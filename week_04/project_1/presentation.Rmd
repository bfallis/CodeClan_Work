---
title: "How do we avoid bias in machine learning algorithms?"
author: "Blair"
date: "24/07/2019"
output: 
  html_document:
    css: styles.css
---

Skyscrapers.

Aeroplanes.

Cars.

Bikes.

Graduation.

Gorillas.

Possibly one of the worst examples of one of the most racist things anyone could ever say. But this wasn't a person captioning these photos in this manner. This was the Google photos app in 2015 which automatically labels images based on what the algorithm determined was in the picture.

One of the possible reasons for this occurring that was suggested is the composition of the training data which may have under-represented people from  minority backgrounds. Another issue raised was the historical problems with camera calibration, that leaves people with darker skin underexposed and difficult to distinguish in images. Both of these would in their own way be examples of bias, sample bias in the training data and measurement bias in the calibration of the cameras used to take photos.

In 2016 Microsoft created Tay, a twitter bot created with the speech patterns of a typical 19 year old american girl. The intention was to allow Tay to interact with other users of twitter and as the bot interacted with more people it would grow more adept at communication.

It took 16 hours for Tay to go from here:

To here.

Tay was switched off after less than 24 hours. Microsoft said this was as a result of a coordinated trolling campaign by some internet forums: an overwhelmingly large amount of input was incredibly toxic, so that's what the output started becoming. Again, bias input in, unexpected results out.

But so what? These examples, while terrible, don't really have any lasting consequences. It's not like Google or Microsoft suffered massively other than maybe a slight loss of reputation but both are still behemoths of industry today.

## Meet Steve.

It was just after sundown one night in 2014 when a man knocked on Steve Talley’s door in south Denver. The man claimed to have hit Talley’s car and asked him to assess the damage. So Steve went outside to take a look.

Seconds later, he was knocked to the pavement. Flash bang grenades detonated, temporarily blinding and deafening him. Three men dressed in black jackets, goggles, and helmets repeatedly hit him with batons and the butts of their guns. After sustaining a <span class="emphasised">broken sternum, several broken teeth, four ruptured spinal disks, blood clots in his right leg, nerve damage in his right ankle, and a possibly fractured penis</span> he was taken to a detention center, where he was arrested for two bank robberies, the last 10 days before his arrest.

He was held for nearly two months in a maximum security jail and was released only after his lawyer obtained his employer’s surveillance records. In a time-stamped audio recording from the day and time of the first robbery, Talley could be heard at his desk where he worked as a financial advisor trying to sell mutual funds to a potential client.

In the 2 months Steve was in jail he lost his job so had no money coming in. He missed his rent payments so was made homeless. Once released his estranged wife refused to allow his children to see him. Steve used to work in financial services, but that was now no longer possible as, when you're all over the intenet for being a bank robber, getting employed in a bank handling people's money becomes a lot trickier than it was previously.

A year later Steve was arrested for a second time for the second of the 2 bank robberies. Despite having a solid alibi the state of Denver took Mr Talley to court and it was only when the bank clerk who dealt with the robber said on the stand under oath that Steve wasn't the guy that robbed the bank was he cleared. By this point Steve's licences to work in finance had expired and so was unable to gain new employment doing what he knew best.

All this happened because a facial recognition system used by the FBI at the time incorrectly matched images of Steve to those of the bank robber, and then that was verified by human FBI agents who blindly followed what the algorithm told them. Observer bias.

## Other uses of ML in society

And this is only one example of how machine learning models affect our every day lives. The Harm Assessment Risk Tool (HART) is being used by Durham Constabulary and is designed to predict whether suspects are at a low, moderate or high risk of committing further crimes in a two years period, similar to the COMPAS system in America. IBM Green Horizon analyzes environmental data from thousands of sensors and sources to predict accurate, evolving weather and pollution forecasts. We have self driving cars, a device in my pocket with a personal virtual assistant I can ask questions of and in AlphaGo, an AI that can defeat the best players in the world at one of the most complex games there is. Almost no part of our lives remains untouches by machine learning and that's why it's so important to get the models correct, part of which is ensuring we feed these models unbiased data.

## Identifying and Combatting Bias

So what are the different types of bias we can encounter?

### Sample bias. 

Where the collected data doesn’t accurately represent the environment the program is expected to run into. The first way we can introduce sample bias include <span class="emphasised">self-selection</span>, where members of a population have to choose to be included in the sample, meaning individuals with strong opinions/feelings on a subject dominate in comparison to those who don't feel as strongly. 

The next would be <span class="emphasised">undercoverage</span>, where we undersample a certain demographic of the population, skewing the results in some way. An example would be training a CCTV image recognition program only using daytime images, if put into production there'd be no guarantee it would be able to function adequately at night time.

<span class="emphasised">Survivorship</span> bias occurs when prior to our sampling there is some selection event in the population which results in our sample not being representative of the original population. Consider successful entrepeneurs, and the desire to copy them to also become successful. It's rare to take away from these people advice on what not to do and that’s because they don’t know. Information like that is lost along with the people who don’t make it out of bad situations.

Finally, <span class="emphasised">response</span> bias can come about due to under or overreporting of metrics in surveys, perhaps due to a wish to conform to societal norms or an unwillingness to be seen as partaking in aberant behaviours.

So given all these type of sample bias, how do we combat them? 2 of the best methods are stratification and weighting. <span class="emphasised">Proportionate stratified random sampling</span> is the process of splitting the population up into different non overlapping strata such that the proportion of each stratum in the sample is the same as the proportion of that stratum in the population. We can them randomly sample within each stratum ensuring our sample keeps the same makeup of the original population. If we were unable to get an accurate sample in this way we could <span class="emphasised">weight</span> any under represented strata, giving more importance to a grouping such that it would feature equally in the outcome of our algorithm.

### Exclusion bias

Comes as a result of <span class="emphasised">excluding some features from our dataset usually under the umbrella of cleaning our data</span>. The researcher applies different criteria to the make up of the control group vs case group, exluding a demographic from one but not the other. Something like a patient being ineligible for a drug trial due to a medical issue, but people with the same undiagnosed condition being in the control group.

We can overcome exclusion bias by thoroughly investigating the features/groups we're thinking of excluding beforehand, having colleagues cast an eye over them or there are tools that can help such as using the caret R package to calculate the relative feature importance. This allows us to easily see which of the variables collected are the most important and if the one we're about to throw out actually matter more than we think.

### Observer bias

Where the <span class="emphasised">person conducting the experiment influences the experiment themselves</span>. The classic example is "Clever Hans", a horse that was said to understand basic maths, tapping out the correct answer to sums with its hoof. The actual reason for the horse getting the answer correct was not understanding maths but by responding to the body language of the questioner.

A lot of experimental methodologies are attempts to limit observer bias, including randomized controlled trials and <span class="emphasised">double blind trials</span>, which should be used whenever possible. In cases where measurements are recorded by human observers rather than by mechanical instruments, it is advisable to train people as observers who have little or no stake in the outcome of the experiment until their recorded observations correlate well on trials.
  
### Prejudice bias

The result of cultural influences or stereotypes. When <span class="emphasised">things that we don’t like in our reality</span> like judging by appearances, social class, status, gender and much more <span class="emphasised">is not fixed in our machine learning model</span>. An example woud be a computer vision program that detects people at work. We feed our model millions of images of where men are coding and women are cooking. The algorithm is likely to learn that coders are men and women are chefs. Which is wrong since anyone can work as anything more or less.

Now really this could be considered in with all the types of bias we've already talked about, meaning the best way to overcome this to expose the algorithm to a more even-handed distribution of examples using the same techniques as before, having double blind trials etc.
  
### Measurement bias

<span class="emphasised">An issue with the device used to observe or measure the experiement</span>. Something like taking images with a camera that automatically increases the exposure. This measurement tool failed to replicate the environment on which the model will operate, in other words, it messed up its training data that it no longer represents real data that it will work on when it’s launched.

Fairly straightforward to eliminate this by having multiple measuring devices and hiring people who are trained to compare the output of these devices.

## Identifying bias in training datasets

But suppose we're given a data set without actually conducting the data collection ourselves. <span class="emphasised">How do we try to make a reasonable judgement on whether the dataset is biased in some way</span>, what should we look for?

We can start with the obvious, are there a <span class="emphasised">large number of records with missing values for certain fields</span>? If there are we should be asking why are so many values missing.

Are there any <span class="emphasised">unexpected values in the dataset</span>, is there anything that seems a wild outlier in comparison to the others results. It might be valid however it's worth asking the question. Are there any criteria that we're measuring against that seem like they don't belong?

Is the dataset skewed in some way? Conduct some simple statistical measures on the dataset to determine how skewed it is from a normal distribution. Visualise the data before doing anything else.

Can you compare your sample with maybe other respresentative samples in experiments conducted by others?

## How to compensate for bias when building models

Even after all that we may be unsure about our dataset but still have to build the model anyway. Fortunately there are a number of frameworks that can be used to determine if our model is biased which I'll go over very briefly here.

Lime ("Local Interpretable Model agnostic Explanations") is a tool which <span class="emphasised">treats the model as a black box and attempts to understand it by perturbing the input of data sample and understanding how the predictions change</span>. So in image classification that might be an image which can be divided into superpixels, clusters of like parts and Lime will then provide an explanation of how the model reached it's conclusion.

FairML is similar in that it it also treats the model it's auditing as a black box and changes the inputs to analyse differences in output and results in quantifying the relative significance of each of the inputs. So which variables in the dataset contribute most to the outcome?

Google's What/If is another free toolkit who's selling point is it allows visual analysis of a model without the need to write any code. This allows anyone to tweak input parameters and see in real time the output of the model.

IBM AI Fairness 360 contains many fairness metrics and bias mitigation algorithms developed by the broader algorithmic fairness research community that can all be called in a standard way to determine model suitability.

Facebook have Fairness Flow, Microsoft are building a toolkit so there should be a variety of ways in future to ensure we have fair models.

## How to monitor model for negative feedback

We've made sure our training dataset is as unbiased as we can make it, we've analysed our model to make sure it's also unbiased, do we now just walk away and onto the next project? Responsible data science says no, we have a duty to ensure the model continues to function as expected and doesn't create any of the negative feedback loops that can be harmful to individuals and society as a whole. 

One possible suggestion is a sister model that takes in as input a time series snapshot of the output of the primary model. As the secondary model generates performance metrics of the first it will create patterns of the output of the first, and any anomolies in the pattern will be detected.

## Conclusion

As shown the consequences of bias in machine learning can have devastating impacts on individuals and society and it's upon us as data scientists of the future to make sure we keep topic of bias at the forefront of our minds when working in this area. 